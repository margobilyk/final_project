{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99efa749-a08f-45a2-bafe-c247bd5a59d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, date_format, hour, dayofweek, month, when, lit, \n",
    "    sin, cos, trim, upper\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import math\n",
    "\n",
    "# --- Setup & Configuration ---\n",
    "dbutils.widgets.text(\"run_date\", \"2025-12-09\")\n",
    "\n",
    "source_table = \"final_project.bronze.crimes_bronze\"\n",
    "target_table = \"final_project.silver.crimes_silver\"\n",
    "\n",
    "# Classification logic for Crime Categories\n",
    "violent_crimes = [\n",
    "    'HOMICIDE', 'CRIM SEXUAL ASSAULT', 'ROBBERY', 'ASSAULT', \n",
    "    'BATTERY', 'KIDNAPPING', 'SEX OFFENSE', 'CRIMINAL SEXUAL ASSAULT'\n",
    "]\n",
    "property_crimes = [\n",
    "    'BURGLARY', 'THEFT', 'MOTOR VEHICLE THEFT', 'ARSON', \n",
    "    'DECEPTIVE PRACTICE', 'CRIMINAL DAMAGE'\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(f\"Reading from Bronze: {source_table}\")\n",
    "    df = spark.read.table(source_table)\n",
    "    initial_count = df.count()\n",
    "    print(f\"Initial rows: {initial_count}\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # STEP 1: Cleaning & Quality Checks\n",
    "    # ==============================================================================\n",
    "    print(\"Performing cleaning...\")\n",
    "    \n",
    "    # 1. Deduplication based on unique identifiers\n",
    "    df_clean = df.dropDuplicates([\"ID\", \"Case_Number\"])\n",
    "    \n",
    "    # 2. Casting types and Filtering Invalid Coordinates\n",
    "    # Chicago Bounding Box approx: Lat 41.6-42.1, Lon -87.9 to -87.5\n",
    "    df_clean = df_clean \\\n",
    "        .withColumn(\"Latitude\", col(\"Latitude\").cast(DoubleType())) \\\n",
    "        .withColumn(\"Longitude\", col(\"Longitude\").cast(DoubleType())) \\\n",
    "        .withColumn(\"District\", col(\"District\").cast(IntegerType())) \\\n",
    "        .filter(\n",
    "            (col(\"Latitude\") > 41.6) & (col(\"Latitude\") < 42.1) &\n",
    "            (col(\"Longitude\") > -87.95) & (col(\"Longitude\") < -87.5)\n",
    "        )\n",
    "    \n",
    "    # 3. Text Standardization (Trimming whitespace)\n",
    "    df_clean = df_clean.withColumn(\"Location_Description\", trim(upper(col(\"Location_Description\"))))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # STEP 2: Feature Engineering (Human Readable)\n",
    "    # ==============================================================================\n",
    "    print(\"Performing Feature Engineering...\")\n",
    "    \n",
    "    # Parsing Timestamp\n",
    "    df_transformed = df_clean.withColumn(\"parsed_timestamp\", to_timestamp(col(\"Date\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    \n",
    "    # Extracting Date Components\n",
    "    df_transformed = df_transformed \\\n",
    "        .withColumn(\"crime_date\", date_format(col(\"parsed_timestamp\"), \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"crime_time\", date_format(col(\"parsed_timestamp\"), \"HH:mm:ss\")) \\\n",
    "        .withColumn(\"crime_hour\", hour(col(\"parsed_timestamp\"))) \\\n",
    "        .withColumn(\"crime_month\", month(col(\"parsed_timestamp\"))) \\\n",
    "        .withColumn(\"day_of_week\", dayofweek(col(\"parsed_timestamp\"))) \\\n",
    "        .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "    # Time of Day Binning (Morning, Afternoon, Evening, Night)\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        \"Time_of_Day\",\n",
    "        when((col(\"crime_hour\") >= 5) & (col(\"crime_hour\") < 12), \"Morning\")\n",
    "        .when((col(\"crime_hour\") >= 12) & (col(\"crime_hour\") < 17), \"Afternoon\")\n",
    "        .when((col(\"crime_hour\") >= 17) & (col(\"crime_hour\") < 21), \"Evening\")\n",
    "        .otherwise(\"Night\")\n",
    "    )\n",
    "\n",
    "    # Crime Severity Grouping (Violent vs Property)\n",
    "    df_transformed = df_transformed.withColumn(\n",
    "        \"Crime_Category\",\n",
    "        when(col(\"Primary_Type\").isin(violent_crimes), \"Violent\")\n",
    "        .when(col(\"Primary_Type\").isin(property_crimes), \"Property\")\n",
    "        .otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # STEP 3: Advanced ML Preparation (Cyclical Features)\n",
    "    # ==============================================================================\n",
    "    print(\"Calculating Cyclical Features for ML...\")\n",
    "    \n",
    "    # Converting linear time (0-23 hours) into circular dimensions (sin/cos)\n",
    "    # This helps models understand that 23:00 is close to 00:00\n",
    "    df_transformed = df_transformed \\\n",
    "        .withColumn(\"hour_sin\", sin(col(\"crime_hour\") * (2 * math.pi / 24))) \\\n",
    "        .withColumn(\"hour_cos\", cos(col(\"crime_hour\") * (2 * math.pi / 24))) \\\n",
    "        .withColumn(\"month_sin\", sin(col(\"crime_month\") * (2 * math.pi / 12))) \\\n",
    "        .withColumn(\"month_cos\", cos(col(\"crime_month\") * (2 * math.pi / 12)))\n",
    "\n",
    "    # Dropping raw/temporary columns\n",
    "    df_final = df_transformed.drop(\"Location\", \"Date\", \"parsed_timestamp\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # STEP 4: Data Storage (Silver Layer)\n",
    "    # ==============================================================================\n",
    "    print(f\"Writing to Silver: {target_table}\")\n",
    "    \n",
    "    # Partitioning by District optimizes downstream queries for specific areas\n",
    "    df_final.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"District\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(target_table)\n",
    "        \n",
    "    final_count = spark.read.table(target_table).count()\n",
    "    print(f\"Silver transformation successful. Final records: {final_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Silver transformation: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Transformation",
   "widgets": {
    "run_date": {
     "currentValue": "2025-12-09",
     "nuid": "9dd99430-d782-4a65-a7e3-360d88077a23",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-12-09",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-12-09",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
