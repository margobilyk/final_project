{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0813bd52-d6e3-4f83-9871-6a9c5f41c04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8bcf87-e775-4447-a154-8714f12d3efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, precision_recall_curve)\n",
    "\n",
    "DATA_TABLE = \"final_project.gold.ml_features\" \n",
    "VOLUME_PATH = \"/Volumes/final_project/graphs/graphs/\"\n",
    "\n",
    "MODEL_FILE = os.path.join(VOLUME_PATH, \"police_model_final.pkl\")\n",
    "METRICS_FILE = os.path.join(VOLUME_PATH, \"model_metrics_final.txt\")\n",
    "SCHEDULE_FILE_PREFIX = os.path.join(VOLUME_PATH, \"patrol_schedule_final_\")\n",
    "\n",
    "BASE_FEATURES = ['hour_sin', 'hour_cos', 'day_of_week', 'month_sin', 'month_cos', 'is_weekend']\n",
    "\n",
    "def run_pipeline():\n",
    "    print(f\"Starting Final Balanced Pipeline using {DATA_TABLE}\")\n",
    "    \n",
    "    try:\n",
    "        df_spark = spark.table(DATA_TABLE)\n",
    "        df = df_spark.toPandas()\n",
    "        print(f\"Loaded {len(df)} records.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    df['District'] = df['District'].astype(int)\n",
    "    \n",
    "    try:\n",
    "        df['Is_Violent'] = df['label_category'].apply(lambda x: 1 if x == 'Violent' else 0)\n",
    "    except KeyError:\n",
    "        print(\"Column 'label_category' not found!\")\n",
    "        return\n",
    "\n",
    "    if 'crime_hour' in df.columns:\n",
    "        df['hour_int'] = df['crime_hour']\n",
    "    else:\n",
    "        df['hour_int'] = 12\n",
    "\n",
    "    df['dist_hour_key'] = df['District'].astype(str) + \"_\" + df['hour_int'].astype(str)\n",
    "    \n",
    "    X = df[BASE_FEATURES + ['District', 'dist_hour_key']]\n",
    "    y = df['Is_Violent']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Applying Target Encoding...\")\n",
    "    \n",
    "    temp_train = X_train.copy()\n",
    "    temp_train['Is_Violent'] = y_train\n",
    "    \n",
    "    global_mean_risk = y_train.mean()\n",
    "\n",
    "    district_risk_map = temp_train.groupby('District')['Is_Violent'].mean().to_dict()\n",
    "    interaction_risk_map = temp_train.groupby('dist_hour_key')['Is_Violent'].mean().to_dict()\n",
    "\n",
    "    def apply_mappings(data_df):\n",
    "        data_df['district_risk'] = data_df['District'].map(district_risk_map).fillna(global_mean_risk)\n",
    "        data_df['interaction_risk'] = data_df['dist_hour_key'].map(interaction_risk_map).fillna(global_mean_risk)\n",
    "        return data_df\n",
    "\n",
    "    X_train = apply_mappings(X_train)\n",
    "    X_test = apply_mappings(X_test)\n",
    "\n",
    "    final_features = BASE_FEATURES + ['district_risk', 'interaction_risk']\n",
    "    \n",
    "    X_train_final = X_train[final_features]\n",
    "    X_test_final = X_test[final_features]\n",
    "\n",
    "    print(\"Step 1: Training Model (Balanced)...\")\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=1, \n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=10, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train_final, y_train)\n",
    "\n",
    "    print(\"Step 2: Metrics Evaluation\")\n",
    "    y_proba = xgb.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    \n",
    "    target_recall = 0.60 \n",
    "    valid_indices = np.where(recalls >= target_recall)[0]\n",
    "    if len(valid_indices) > 0:\n",
    "        best_idx = valid_indices[np.argmax(precisions[valid_indices])]\n",
    "        optimal_threshold = thresholds[best_idx]\n",
    "    else:\n",
    "        optimal_threshold = 0.5\n",
    "\n",
    "    y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "ACCURACY:  {acc:.4f}\n",
    "PRECISION: {prec:.4f}\n",
    "RECALL:    {rec:.4f}\n",
    "F1_SCORE:  {f1:.4f}\n",
    "ROC_AUC:   {auc:.4f}\n",
    "THRESHOLD: {optimal_threshold:.4f}\n",
    "\"\"\"\n",
    "    print(report)\n",
    "\n",
    "    with open(METRICS_FILE, \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    artifact = {\n",
    "        \"model\": xgb,\n",
    "        \"threshold\": optimal_threshold,\n",
    "        \"features\": final_features,\n",
    "        \"district_risk_map\": district_risk_map,\n",
    "        \"interaction_risk_map\": interaction_risk_map,\n",
    "        \"global_mean_risk\": global_mean_risk,\n",
    "        \"version\": \"xgboost_final\"\n",
    "    }\n",
    "\n",
    "    joblib.dump(artifact, MODEL_FILE)\n",
    "    print(f\"Model saved to {MODEL_FILE}\")\n",
    "\n",
    "    print(\"Step 3: Generating Schedule\")\n",
    "    start_time = datetime.now().replace(minute=0, second=0, microsecond=0)\n",
    "    future_rows = []\n",
    "    districts = sorted(df['District'].unique().tolist())\n",
    "\n",
    "    for h in range(24):\n",
    "        curr = start_time + timedelta(hours=h)\n",
    "        py_day = curr.weekday()\n",
    "        spark_day = 1 if py_day == 6 else py_day + 2\n",
    "        is_weekend = 1 if py_day in [5, 6] else 0\n",
    "        current_hour = curr.hour\n",
    "        \n",
    "        for dist in districts:\n",
    "            dist_key = str(dist) + \"_\" + str(current_hour)\n",
    "            dist_risk = district_risk_map.get(dist, global_mean_risk)\n",
    "            inter_risk = interaction_risk_map.get(dist_key, global_mean_risk)\n",
    "            \n",
    "            future_rows.append({\n",
    "                'hour_sin': np.sin(2 * np.pi * current_hour / 24),\n",
    "                'hour_cos': np.cos(2 * np.pi * current_hour / 24),\n",
    "                'day_of_week': spark_day,\n",
    "                'month_sin': np.sin(2 * np.pi * curr.month / 12),\n",
    "                'month_cos': np.cos(2 * np.pi * curr.month / 12),\n",
    "                'is_weekend': is_weekend,\n",
    "                'district_risk': dist_risk,\n",
    "                'interaction_risk': inter_risk,\n",
    "                'Timestamp': curr,\n",
    "                'District_ID': dist\n",
    "            })\n",
    "\n",
    "    df_batch = pd.DataFrame(future_rows)\n",
    "    X_batch = df_batch[final_features]\n",
    "    probs = xgb.predict_proba(X_batch)[:, 1]\n",
    "\n",
    "    df_batch['Risk_Probability'] = probs\n",
    "    df_batch['Action'] = np.where(probs > optimal_threshold, \"DISPATCH\", \"MONITOR\")\n",
    "\n",
    "    schedule_filename = f\"{SCHEDULE_FILE_PREFIX}{start_time.date()}.csv\"\n",
    "    output_cols = ['Timestamp', 'District_ID', 'Risk_Probability', 'Action']\n",
    "    \n",
    "    df_batch[df_batch['Action'] == \"DISPATCH\"][output_cols] \\\n",
    "        .sort_values(by='Risk_Probability', ascending=False) \\\n",
    "        .to_csv(schedule_filename, index=False)\n",
    "\n",
    "    print(f\"Schedule saved: {schedule_filename}\")\n",
    "    print(\"Pipeline Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
